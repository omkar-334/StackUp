{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4e39bff",
   "metadata": {},
   "source": [
    "# Introduction to NLP in Python\n",
    "## Quest 2: Vectorization\n",
    "\n",
    "### Bag-of-Words Model (BOW)\n",
    "\n",
    "\n",
    "In Python, we can use the scikit-learn library to create BoW vectors from text data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2cb2711",
   "metadata": {},
   "source": [
    "1. If it has not been installed, run the following line to install scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977ec549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0377801",
   "metadata": {},
   "source": [
    "2. Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0769fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8d5289f",
   "metadata": {},
   "source": [
    "3. Create an instance of the CountVectorizer class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c684896",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect= CountVectorizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36e3ca6f",
   "metadata": {},
   "source": [
    "4. Load your text data into a list or an array. Here, we have provided some sentences for you to work with. Feel free to add on to the sentences or come up with your own sample text to experiment more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57875cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"Data science involves using statistical and computational techniques to extract insights from data.\",\n",
    "    \"In data science, machine learning algorithms are used to build predictive models based on data.\",\n",
    "    \"Big data technologies enable data scientists to process and analyze large volumes of data more efficiently\",\n",
    "    \"Data visualization is an important part of data science, as it helps to communicate insights effectively.\",\n",
    "    \"Natural language processing is a key area of data science that focuses on understanding and processing human language.\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6256d9b4",
   "metadata": {},
   "source": [
    "5. Next, we fit and transform the text data. Fitting the vectorizer on the text data creates the vocabulary, while the transform method transforms the text data into a BoW representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc9f926c",
   "metadata": {},
   "source": [
    "The resulting `bow` object is a sparse matrix that represents the text data in BoW form. Go ahead and print the contents of this matrix to see how the words in the sample text have been mapped to their respective indices in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac4bf6e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   algorithms  an  analyze  and  are  area  as  based  big  build  ...   \n",
      "0           0   0        0    1    0     0   0      0    0      0  ...  \\\n",
      "1           1   0        0    0    1     0   0      1    0      1  ...   \n",
      "2           0   0        1    1    0     0   0      0    1      0  ...   \n",
      "3           0   1        0    0    0     0   1      0    0      0  ...   \n",
      "4           0   0        0    1    0     1   0      0    0      0  ...   \n",
      "\n",
      "   statistical  techniques  technologies  that  to  understanding  used   \n",
      "0            1           1             0     0   1              0     0  \\\n",
      "1            0           0             0     0   1              0     1   \n",
      "2            0           0             1     0   1              0     0   \n",
      "3            0           0             0     0   1              0     0   \n",
      "4            0           0             0     1   0              1     0   \n",
      "\n",
      "   using  visualization  volumes  \n",
      "0      1              0        0  \n",
      "1      0              0        0  \n",
      "2      0              0        1  \n",
      "3      0              1        0  \n",
      "4      0              0        0  \n",
      "\n",
      "[5 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "count_matrix = count_vect.fit_transform(text)\n",
    "count_array = count_matrix.toarray()\n",
    "df = pd.DataFrame(data=count_array,columns = count_vect.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8e67d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['algorithms', 'an', 'analyze', 'and', 'are', 'area', 'as', 'based',\n",
       "       'big', 'build', 'communicate', 'computational', 'data',\n",
       "       'effectively', 'efficiently', 'enable', 'extract', 'focuses',\n",
       "       'from', 'helps', 'human', 'important', 'in', 'insights',\n",
       "       'involves', 'is', 'it', 'key', 'language', 'large', 'learning',\n",
       "       'machine', 'models', 'more', 'natural', 'of', 'on', 'part',\n",
       "       'predictive', 'process', 'processing', 'science', 'scientists',\n",
       "       'statistical', 'techniques', 'technologies', 'that', 'to',\n",
       "       'understanding', 'used', 'using', 'visualization', 'volumes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91649d6a",
   "metadata": {},
   "source": [
    "Each row in the matrix corresponds to a sentence in the vocabulary. The values in the matrix represent the frequency of each word in each sentence. \n",
    "\n",
    "That's it! You now know how to use the Bag-of-Words vectorizer in Python. **Head back to the StackUp platform, where we continue with another vectorization technique.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2bb47ea",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "The scikit-learn library provides a convenient implementation of the TF-IDF algorithm. \n",
    "1. First, run the cell below to import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46736703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70e2365b",
   "metadata": {},
   "source": [
    "2. Define the text that you want to analyze. This can be a list or array of strings, or even a Pandas DataFrame containing text data for a larger dataset. \n",
    "\n",
    "Here, lets work with a slightly larger set of data than before. Feel free to include more sentences or import your own Pandas DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3619e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    \"Supervised learning is a type of machine learning.\",\n",
    "    \"Unsupervised learning is another type of machine learning.\",\n",
    "    \"Machine learning algorithms can be used for a wide range of applications, such as image recognition and natural language processing.\",\n",
    "    \"One of the most popular machine learning algorithms is the decision tree.\",\n",
    "    \"Random forest is an ensemble learning method that combines multiple decision trees.\",\n",
    "    \"Support vector machines are a powerful class of machine learning algorithms used for classification and regression tasks.\",\n",
    "    \"Neural networks are a type of machine learning algorithm inspired by the structure and function of the human brain.\",\n",
    "    \"Deep learning is a subfield of machine learning that involves neural networks with many layers.\",\n",
    "    \"Transfer learning is a technique in machine learning where a model trained on one task is used to improve performance on another related task.\",\n",
    "    \"Natural Language Processing (NLP) is a subfield of machine learning that deals with the interaction between computers and humans in natural language.\",\n",
    "    \"Text classification is an important task in NLP that involves assigning a document to one or more predefined categories.\",\n",
    "    \"Named Entity Recognition (NER) is another important task in NLP that involves identifying and extracting entities such as people, organizations, and locations from text data.\"\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96b5cef4",
   "metadata": {},
   "source": [
    "3. Create an instance of the TfidfVectorizer class. You can specify any parameters you want to use. Some common parameters include:\n",
    "- `max_features`: the maximum number of features (unique words) to include in the TF-IDF matrix\n",
    "- `stop_words`: a list of words to exclude from the TF-IDF calculation, such as common stop words like \"the\" and \"and\"\n",
    "\n",
    "If you are keen to know more, check out other available parameters [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Feel free to experiment with the parameters and changing the values in the vectorizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6d3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc6f65fa",
   "metadata": {},
   "source": [
    "4. Fit and transform the text into a TF-IDF matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90a89793",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.fit_transform(text_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3615f05e",
   "metadata": {},
   "source": [
    "5. Print the resulting matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd233e02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.52146104 0.         0.28202368 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.64158678\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.48673138 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.52146104 0.         0.28202368 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.48673138 0.64158678 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.26082995 0.34381397 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.34381397 0.         0.         0.         0.         0.\n",
      "  0.29527143 0.         0.13972045 0.         0.15113105 0.\n",
      "  0.         0.         0.         0.         0.29527143 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.29527143 0.         0.34381397\n",
      "  0.29527143 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.26082995\n",
      "  0.         0.34381397]\n",
      " [0.         0.39592654 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.44820694 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.21208851 0.         0.22940921 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.52189204 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.52189204 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35572313 0.         0.         0.\n",
      "  0.30549916 0.         0.         0.35572313 0.         0.\n",
      "  0.         0.35572313 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14456014 0.         0.         0.\n",
      "  0.35572313 0.         0.35572313 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.35572313 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35572313 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.24947888 0.         0.         0.         0.\n",
      "  0.32885151 0.28242149 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.13363995 0.         0.14455396 0.32885151\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.32885151 0.         0.         0.         0.\n",
      "  0.         0.32885151 0.         0.         0.         0.\n",
      "  0.32885151 0.         0.32885151 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.24947888\n",
      "  0.32885151 0.        ]\n",
      " [0.34484776 0.         0.         0.         0.34484776 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.34484776 0.34484776 0.         0.\n",
      "  0.         0.         0.         0.34484776 0.         0.\n",
      "  0.         0.         0.14014057 0.         0.15158547 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.29615926 0.29615926 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.34484776 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.26161422 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.42100068 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31938664\n",
      "  0.         0.42100068 0.34217577 0.         0.18506018 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.36156028 0.36156028 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.36156028 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30526042 0.         0.         0.\n",
      "  0.         0.         0.24810583 0.         0.13418398 0.\n",
      "  0.         0.30526042 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.30526042\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30526042 0.         0.         0.\n",
      "  0.         0.46316362 0.         0.30526042 0.         0.30526042\n",
      "  0.30526042 0.         0.         0.         0.         0.23158181\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.28502321 0.         0.28502321\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.28502321 0.\n",
      "  0.         0.         0.         0.         0.28502321 0.\n",
      "  0.48956249 0.         0.11582884 0.         0.12528827 0.\n",
      "  0.         0.         0.         0.         0.48956249 0.\n",
      "  0.         0.         0.21622911 0.         0.         0.\n",
      "  0.         0.         0.         0.24478124 0.         0.\n",
      "  0.         0.         0.         0.         0.24478124 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.35490322 0.         0.35490322\n",
      "  0.         0.30479501 0.         0.         0.         0.\n",
      "  0.         0.         0.35490322 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30479501 0.         0.         0.         0.26924266\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.26924266 0.         0.         0.\n",
      "  0.         0.         0.35490322 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.26924266 0.         0.         0.30479501 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26784288 0.\n",
      "  0.         0.         0.         0.         0.26784288 0.26784288\n",
      "  0.26784288 0.         0.         0.         0.         0.26784288\n",
      "  0.         0.23002658 0.         0.         0.         0.20319549\n",
      "  0.         0.         0.         0.26784288 0.         0.\n",
      "  0.         0.         0.         0.26784288 0.         0.26784288\n",
      "  0.         0.         0.20319549 0.26784288 0.26784288 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.23002658 0.         0.         0.         0.         0.\n",
      "  0.         0.20319549 0.         0.         0.23002658 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59902e55",
   "metadata": {},
   "source": [
    "The resulting matrix is printed using the `toarray()` method, which converts the sparse matrix to a dense matrix for easier viewing.\n",
    "\n",
    "That's it for the TF-IDF technique! **Switch back over to the StackUp platform, where we cover the third and final vectorization method.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03e18e2a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Word Embeddings\n",
    "\n",
    "Here, we will use the Gensim Python library to create word embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cd265ca",
   "metadata": {},
   "source": [
    "1. If it has not been installed, run the following line to install Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f33b6084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.9.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e3091de",
   "metadata": {},
   "source": [
    "2. Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0daf62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19d2dee7",
   "metadata": {},
   "source": [
    "3. Train a Word2Vec model using a tokenized text corpus. Word2Vec takes in tokens as its input, so the `word_tokenize` method from the previous quest is used here. We will make use of the `text_data` list from the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94cefa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    \"Supervised learning is a type of machine learning.\",\n",
    "    \"Unsupervised learning is another type of machine learning.\",\n",
    "    \"Machine learning algorithms can be used for a wide range of applications, such as image recognition and natural language processing.\",\n",
    "    \"One of the most popular machine learning algorithms is the decision tree.\",\n",
    "    \"Random forest is an ensemble learning method that combines multiple decision trees.\",\n",
    "    \"Support vector machines are a powerful class of machine learning algorithms used for classification and regression tasks.\",\n",
    "    \"Neural networks are a type of machine learning algorithm inspired by the structure and function of the human brain.\",\n",
    "    \"Deep learning is a subfield of machine learning that involves neural networks with many layers.\",\n",
    "    \"Transfer learning is a technique in machine learning where a model trained on one task is used to improve performance on another related task.\",\n",
    "    \"Natural Language Processing (NLP) is a subfield of machine learning that deals with the interaction between computers and humans in natural language.\",\n",
    "    \"Text classification is an important task in NLP that involves assigning a document to one or more predefined categories.\",\n",
    "    \"Named Entity Recognition (NER) is another important task in NLP that involves identifying and extracting entities such as people, organizations, and locations from text data.\"\n",
    "]\n",
    "\n",
    "tokens_in_text = [nltk.word_tokenize(sentence.lower()) for sentence in text_data] \n",
    "\n",
    "model = Word2Vec(tokens_in_text, min_count=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3b81db0",
   "metadata": {},
   "source": [
    "The cell above returns the list of sentences into individual tokens after converting the words to lowercase, and is passed into the Word2Vec model. \n",
    "\n",
    "The `min_count` parameter ignores all words with a total frequency lower than this. In this case, all words that only appear once is ignored."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f25d7921",
   "metadata": {},
   "source": [
    "4. Once your model is trained, you can access the word embeddings for any word like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2066d4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.2433320e-03  4.1890572e-04 -9.6239574e-04 -3.2622248e-03\n",
      "  4.9813832e-03 -9.1374218e-03  5.1592113e-03  5.7440903e-03\n",
      " -2.4666542e-03 -4.2066104e-03 -7.3827682e-03 -6.4108432e-03\n",
      " -5.1381472e-03  3.1858739e-03 -4.7803563e-03 -1.2228384e-03\n",
      " -8.9032417e-03 -3.5037154e-03 -8.0953800e-04 -8.0476720e-03\n",
      "  3.2537808e-03  9.7621465e-03  4.4832961e-03  1.8484986e-03\n",
      " -4.8418064e-03 -1.1118491e-03  2.2252002e-03  1.7033379e-03\n",
      "  8.7847020e-03 -9.2013582e-04  7.0528258e-03  5.7957778e-03\n",
      "  3.1277214e-03  7.2565642e-03 -4.9505257e-03 -3.9364714e-03\n",
      "  6.9563957e-03  8.5821832e-03  3.1465974e-03 -4.1753957e-03\n",
      "  7.6282239e-03  2.6987009e-03 -2.6221786e-04  8.8245040e-03\n",
      "  4.3581417e-03  3.9333361e-03 -3.9797589e-05 -8.9332564e-03\n",
      " -3.0806521e-03  2.4464622e-03 -5.6178658e-03  7.0744818e-03\n",
      "  7.4318070e-03 -8.9356452e-03 -5.3773653e-03 -6.0308459e-03\n",
      " -6.4305880e-04 -1.8272620e-03 -2.0672381e-03 -2.2206558e-03\n",
      "  3.5930267e-03  9.5866909e-03 -4.5388569e-03 -8.2720798e-03\n",
      "  3.7101971e-03 -6.3875900e-03  6.6857054e-03  2.6999521e-03\n",
      " -7.9799686e-03 -7.7022929e-03 -9.2647877e-03  2.4025515e-03\n",
      "  2.7865313e-03  2.6320789e-03  1.9728293e-04  1.0295683e-03\n",
      "  3.0974834e-03 -1.5389993e-03 -8.0493242e-03 -6.2121963e-04\n",
      "  3.7043523e-03 -2.6478982e-03 -2.1919645e-03 -2.2312822e-03\n",
      "  7.2559710e-03 -3.0972599e-03  9.5456075e-03 -3.8740141e-03\n",
      " -9.9790515e-03 -6.5089334e-03 -4.3607019e-03  1.2760976e-03\n",
      "  5.1270239e-03  2.1124743e-03  9.1222869e-03  9.1652921e-04\n",
      "  5.6929351e-03  1.3505618e-04 -2.0971072e-03  6.1982679e-03]\n"
     ]
    }
   ],
   "source": [
    "vector = model.wv['class']\n",
    "print(vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6e5f80a",
   "metadata": {},
   "source": [
    "This will return a vector representing the word 'class' in the embedding space. The attribute `wv` of a Word2Vec model stands for \"word vector\".\n",
    "\n",
    "And that sums up the 3 techniques for vectorization in NLP! **Return back to the StackUp platform,** where we wrap up the quest and prepare the deliverables for submission. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
